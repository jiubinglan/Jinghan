***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_flowers_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: D:\Code\Data
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: D:\Code\Data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_flowers_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.3.1+cu121
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 11 ×¨Òµ°æ
GCC version: Could not collect
Clang version: Could not collect
CMake version: Could not collect
Libc version: N/A

Python version: 3.8.19 (default, Mar 20 2024, 19:55:45) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)
Python platform: Windows-10-10.0.22631-SP0
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4070 Ti SUPER
Nvidia driver version: 555.99
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture=9
CurrentClockSpeed=3500
DeviceID=CPU0
Family=205
L2CacheSize=20480
L2CacheSpeed=
Manufacturer=GenuineIntel
MaxClockSpeed=3500
Name=13th Gen Intel(R) Core(TM) i5-13600KF
ProcessorType=3
Revision=

Versions of relevant libraries:
[pip3] flake8==3.7.9
[pip3] numpy==1.24.3
[pip3] torch==2.3.1+cu121
[pip3] torchaudio==2.3.1+cu121
[pip3] torchvision==0.18.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              h74a9793_1  
[conda] mkl                       2021.4.0                 pypi_0    pypi
[conda] mkl-service               2.4.0            py38h2bbff1b_0  
[conda] mkl_fft                   1.3.1            py38h277e83a_0  
[conda] mkl_random                1.2.2            py38hf11a4ad_0  
[conda] numpy                     1.24.3           py38hf95b240_0  
[conda] numpy-base                1.24.3           py38h005ec55_0  
[conda] pytorch-mutex             1.0                         cpu    pytorch
[conda] torch                     2.3.1+cu121              pypi_0    pypi
[conda] torchaudio                2.3.1+cu121              pypi_0    pypi
[conda] torchvision               0.18.1                 py38_cpu    pytorch
        Pillow (10.3.0)

Loading trainer: TCP
Loading dataset: OxfordFlowers
Reading split from D:\Code\Data\oxford_flowers\split_zhou_OxfordFlowers.json
Creating a 16-shot dataset
Creating a 4-shot dataset
Saving preprocessed few-shot data to D:\Code\Data\oxford_flowers\split_fewshot\shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      204
# test     1,053
---------  -------------
['pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea', 'english marigold', 'tiger lily', 'moon orchid', 'bird of paradise', 'monkshood', 'globe thistle', 'snapdragon', "colt's foot", 'king protea', 'spear thistle', 'yellow iris', 'globe-flower', 'purple coneflower', 'peruvian lily', 'balloon flower', 'giant white arum lily', 'fire lily', 'pincushion flower', 'fritillary', 'red ginger', 'grape hyacinth', 'corn poppy', 'prince of wales feathers', 'stemless gentian', 'artichoke', 'sweet william', 'carnation', 'garden phlox', 'love in the mist', 'mexican aster', 'alpine sea holly', 'ruby-lipped cattleya', 'cape flower', 'great masterwort', 'siam tulip', 'lenten rose', 'barbeton daisy', 'daffodil', 'sword lily', 'poinsettia', 'bolero deep blue', 'wallflower', 'marigold', 'buttercup', 'oxeye daisy', 'common dandelion', 'petunia']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X pink primrose, a type of flower.', 'X X X X hard-leaved pocket orchid, a type of flower.', 'X X X X canterbury bells, a type of flower.', 'X X X X sweet pea, a type of flower.', 'X X X X english marigold, a type of flower.', 'X X X X tiger lily, a type of flower.', 'X X X X moon orchid, a type of flower.', 'X X X X bird of paradise, a type of flower.', 'X X X X monkshood, a type of flower.', 'X X X X globe thistle, a type of flower.', 'X X X X snapdragon, a type of flower.', "X X X X colt's foot, a type of flower.", 'X X X X king protea, a type of flower.', 'X X X X spear thistle, a type of flower.', 'X X X X yellow iris, a type of flower.', 'X X X X globe-flower, a type of flower.', 'X X X X purple coneflower, a type of flower.', 'X X X X peruvian lily, a type of flower.', 'X X X X balloon flower, a type of flower.', 'X X X X giant white arum lily, a type of flower.', 'X X X X fire lily, a type of flower.', 'X X X X pincushion flower, a type of flower.', 'X X X X fritillary, a type of flower.', 'X X X X red ginger, a type of flower.', 'X X X X grape hyacinth, a type of flower.', 'X X X X corn poppy, a type of flower.', 'X X X X prince of wales feathers, a type of flower.', 'X X X X stemless gentian, a type of flower.', 'X X X X artichoke, a type of flower.', 'X X X X sweet william, a type of flower.', 'X X X X carnation, a type of flower.', 'X X X X garden phlox, a type of flower.', 'X X X X love in the mist, a type of flower.', 'X X X X mexican aster, a type of flower.', 'X X X X alpine sea holly, a type of flower.', 'X X X X ruby-lipped cattleya, a type of flower.', 'X X X X cape flower, a type of flower.', 'X X X X great masterwort, a type of flower.', 'X X X X siam tulip, a type of flower.', 'X X X X lenten rose, a type of flower.', 'X X X X barbeton daisy, a type of flower.', 'X X X X daffodil, a type of flower.', 'X X X X sword lily, a type of flower.', 'X X X X poinsettia, a type of flower.', 'X X X X bolero deep blue, a type of flower.', 'X X X X wallflower, a type of flower.', 'X X X X marigold, a type of flower.', 'X X X X buttercup, a type of flower.', 'X X X X oxeye daisy, a type of flower.', 'X X X X common dandelion, a type of flower.', 'X X X X petunia, a type of flower.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_flowers_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3\tensorboard)
epoch [1/50] batch [5/25] time 0.052 (5.364) data 0.000 (5.257) loss 3.1621 (3.0629) acc 53.1250 (58.1250) lr 1.0000e-05 eta 1:51:18
epoch [1/50] batch [10/25] time 0.050 (2.708) data 0.000 (2.629) loss 3.2656 (3.0439) acc 50.0000 (57.5000) lr 1.0000e-05 eta 0:55:57
epoch [1/50] batch [15/25] time 0.050 (1.822) data 0.000 (1.752) loss 2.8242 (2.9831) acc 68.7500 (60.0000) lr 1.0000e-05 eta 0:37:30
epoch [1/50] batch [20/25] time 0.051 (1.380) data 0.000 (1.314) loss 2.9805 (2.9688) acc 62.5000 (59.5312) lr 1.0000e-05 eta 0:28:17
epoch [1/50] batch [25/25] time 0.051 (1.114) data 0.000 (1.051) loss 2.9336 (2.9727) acc 62.5000 (59.7500) lr 2.0000e-03 eta 0:22:44
epoch [2/50] batch [5/25] time 0.051 (5.338) data 0.000 (5.223) loss 2.1387 (2.1289) acc 59.3750 (65.0000) lr 2.0000e-03 eta 1:48:31
epoch [2/50] batch [10/25] time 0.052 (2.695) data 0.000 (2.611) loss 1.8574 (1.8996) acc 62.5000 (65.6250) lr 2.0000e-03 eta 0:54:34
epoch [2/50] batch [15/25] time 0.052 (1.814) data 0.001 (1.741) loss 1.0898 (1.7887) acc 78.1250 (65.6250) lr 2.0000e-03 eta 0:36:34
epoch [2/50] batch [20/25] time 0.052 (1.373) data 0.001 (1.306) loss 1.2266 (1.7226) acc 68.7500 (64.8438) lr 2.0000e-03 eta 0:27:34
epoch [2/50] batch [25/25] time 0.051 (1.109) data 0.000 (1.045) loss 1.4346 (1.6372) acc 62.5000 (65.2500) lr 1.9980e-03 eta 0:22:11
epoch [3/50] batch [5/25] time 0.057 (5.338) data 0.000 (5.218) loss 1.1426 (1.3029) acc 71.8750 (67.5000) lr 1.9980e-03 eta 1:46:18
epoch [3/50] batch [10/25] time 0.052 (2.695) data 0.000 (2.609) loss 1.0693 (1.2268) acc 81.2500 (71.5625) lr 1.9980e-03 eta 0:53:27
epoch [3/50] batch [15/25] time 0.049 (1.815) data 0.000 (1.739) loss 0.8633 (1.1883) acc 87.5000 (72.0833) lr 1.9980e-03 eta 0:35:50
epoch [3/50] batch [20/25] time 0.052 (1.374) data 0.000 (1.304) loss 1.2656 (1.1932) acc 78.1250 (72.0312) lr 1.9980e-03 eta 0:27:00
epoch [3/50] batch [25/25] time 0.054 (1.110) data 0.000 (1.044) loss 0.9111 (1.1671) acc 87.5000 (72.5000) lr 1.9921e-03 eta 0:21:44
epoch [4/50] batch [5/25] time 0.059 (5.320) data 0.000 (5.220) loss 0.8784 (0.9133) acc 81.2500 (81.2500) lr 1.9921e-03 eta 1:43:44
epoch [4/50] batch [10/25] time 0.049 (2.685) data 0.000 (2.610) loss 1.2500 (1.0327) acc 68.7500 (77.1875) lr 1.9921e-03 eta 0:52:08
epoch [4/50] batch [15/25] time 0.050 (1.807) data 0.000 (1.740) loss 1.2129 (0.9998) acc 71.8750 (78.5417) lr 1.9921e-03 eta 0:34:56
epoch [4/50] batch [20/25] time 0.054 (1.368) data 0.000 (1.305) loss 0.9146 (0.9795) acc 78.1250 (79.0625) lr 1.9921e-03 eta 0:26:20
epoch [4/50] batch [25/25] time 0.051 (1.105) data 0.000 (1.044) loss 1.0273 (0.9735) acc 78.1250 (78.7500) lr 1.9823e-03 eta 0:21:10
epoch [5/50] batch [5/25] time 0.057 (5.306) data 0.001 (5.234) loss 1.0264 (0.8003) acc 87.5000 (89.3750) lr 1.9823e-03 eta 1:41:15
epoch [5/50] batch [10/25] time 0.055 (2.681) data 0.000 (2.617) loss 0.7158 (0.9129) acc 93.7500 (85.0000) lr 1.9823e-03 eta 0:50:55
epoch [5/50] batch [15/25] time 0.052 (1.805) data 0.000 (1.745) loss 0.7817 (0.9218) acc 84.3750 (85.0000) lr 1.9823e-03 eta 0:34:08
epoch [5/50] batch [20/25] time 0.051 (1.367) data 0.001 (1.309) loss 0.8081 (0.8954) acc 90.6250 (85.4688) lr 1.9823e-03 eta 0:25:44
epoch [5/50] batch [25/25] time 0.051 (1.103) data 0.000 (1.047) loss 0.7710 (0.8817) acc 90.6250 (85.6250) lr 1.9686e-03 eta 0:20:41
epoch [6/50] batch [5/25] time 0.052 (5.308) data 0.000 (5.218) loss 0.8135 (0.8066) acc 90.6250 (84.3750) lr 1.9686e-03 eta 1:39:05
epoch [6/50] batch [10/25] time 0.055 (2.681) data 0.000 (2.609) loss 0.5088 (0.7332) acc 96.8750 (87.8125) lr 1.9686e-03 eta 0:49:49
epoch [6/50] batch [15/25] time 0.056 (1.806) data 0.000 (1.739) loss 1.0410 (0.7543) acc 75.0000 (87.0833) lr 1.9686e-03 eta 0:33:24
epoch [6/50] batch [20/25] time 0.052 (1.368) data 0.001 (1.305) loss 0.6836 (0.7759) acc 84.3750 (85.3125) lr 1.9686e-03 eta 0:25:11
epoch [6/50] batch [25/25] time 0.052 (1.105) data 0.000 (1.044) loss 0.7661 (0.7544) acc 81.2500 (86.0000) lr 1.9511e-03 eta 0:20:15
epoch [7/50] batch [5/25] time 0.051 (5.324) data 0.000 (5.235) loss 0.6260 (0.6593) acc 93.7500 (89.3750) lr 1.9511e-03 eta 1:37:09
epoch [7/50] batch [10/25] time 0.050 (2.687) data 0.000 (2.617) loss 0.8862 (0.6826) acc 78.1250 (88.7500) lr 1.9511e-03 eta 0:48:49
epoch [7/50] batch [15/25] time 0.049 (1.809) data 0.000 (1.745) loss 0.8447 (0.6764) acc 84.3750 (88.5417) lr 1.9511e-03 eta 0:32:42
epoch [7/50] batch [20/25] time 0.050 (1.369) data 0.000 (1.309) loss 0.9136 (0.6809) acc 81.2500 (88.4375) lr 1.9511e-03 eta 0:24:38
epoch [7/50] batch [25/25] time 0.052 (1.106) data 0.001 (1.047) loss 0.6543 (0.6744) acc 90.6250 (88.5000) lr 1.9298e-03 eta 0:19:48
epoch [8/50] batch [5/25] time 0.050 (5.329) data 0.001 (5.227) loss 0.7827 (0.6949) acc 81.2500 (88.7500) lr 1.9298e-03 eta 1:35:01
epoch [8/50] batch [10/25] time 0.055 (2.693) data 0.000 (2.614) loss 0.4456 (0.6625) acc 96.8750 (89.0625) lr 1.9298e-03 eta 0:47:47
epoch [8/50] batch [15/25] time 0.055 (1.813) data 0.000 (1.743) loss 0.5781 (0.6439) acc 90.6250 (90.0000) lr 1.9298e-03 eta 0:32:02
epoch [8/50] batch [20/25] time 0.068 (1.375) data 0.001 (1.307) loss 0.5918 (0.6290) acc 90.6250 (90.4688) lr 1.9298e-03 eta 0:24:10
epoch [8/50] batch [25/25] time 0.051 (1.110) data 0.000 (1.046) loss 0.7061 (0.6348) acc 90.6250 (89.8750) lr 1.9048e-03 eta 0:19:25
epoch [9/50] batch [5/25] time 0.051 (5.298) data 0.001 (5.231) loss 0.7402 (0.7030) acc 87.5000 (88.7500) lr 1.9048e-03 eta 1:32:16
epoch [9/50] batch [10/25] time 0.051 (2.674) data 0.001 (2.616) loss 0.5010 (0.6587) acc 93.7500 (89.3750) lr 1.9048e-03 eta 0:46:21
epoch [9/50] batch [15/25] time 0.063 (1.801) data 0.001 (1.744) loss 0.6611 (0.6471) acc 90.6250 (88.7500) lr 1.9048e-03 eta 0:31:03
epoch [9/50] batch [20/25] time 0.051 (1.363) data 0.000 (1.308) loss 0.7954 (0.6547) acc 87.5000 (88.7500) lr 1.9048e-03 eta 0:23:24
epoch [9/50] batch [25/25] time 0.054 (1.102) data 0.000 (1.046) loss 0.5469 (0.6545) acc 96.8750 (89.1250) lr 1.8763e-03 eta 0:18:49
epoch [10/50] batch [5/25] time 0.052 (5.286) data 0.000 (5.224) loss 0.5234 (0.7039) acc 96.8750 (88.7500) lr 1.8763e-03 eta 1:29:51
epoch [10/50] batch [10/25] time 0.061 (2.670) data 0.001 (2.612) loss 0.7847 (0.6733) acc 84.3750 (89.6875) lr 1.8763e-03 eta 0:45:09
epoch [10/50] batch [15/25] time 0.050 (1.797) data 0.001 (1.741) loss 0.5381 (0.6731) acc 93.7500 (88.9583) lr 1.8763e-03 eta 0:30:14
epoch [10/50] batch [20/25] time 0.051 (1.360) data 0.000 (1.306) loss 0.6055 (0.6531) acc 93.7500 (90.3125) lr 1.8763e-03 eta 0:22:47
epoch [10/50] batch [25/25] time 0.054 (1.099) data 0.000 (1.045) loss 0.4534 (0.6288) acc 90.6250 (90.6250) lr 1.8443e-03 eta 0:18:18
epoch [11/50] batch [5/25] time 0.054 (5.360) data 0.001 (5.232) loss 0.5254 (0.5898) acc 96.8750 (91.8750) lr 1.8443e-03 eta 1:28:53
epoch [11/50] batch [10/25] time 0.051 (2.706) data 0.001 (2.616) loss 0.4185 (0.5920) acc 90.6250 (90.3125) lr 1.8443e-03 eta 0:44:38
epoch [11/50] batch [15/25] time 0.051 (1.821) data 0.000 (1.744) loss 0.4646 (0.5475) acc 96.8750 (92.2917) lr 1.8443e-03 eta 0:29:54
epoch [11/50] batch [20/25] time 0.056 (1.380) data 0.000 (1.308) loss 0.5566 (0.5535) acc 90.6250 (92.5000) lr 1.8443e-03 eta 0:22:31
epoch [11/50] batch [25/25] time 0.056 (1.115) data 0.000 (1.047) loss 0.5820 (0.5541) acc 90.6250 (92.2500) lr 1.8090e-03 eta 0:18:06
epoch [12/50] batch [5/25] time 0.050 (5.318) data 0.000 (5.231) loss 0.7358 (0.5824) acc 87.5000 (91.8750) lr 1.8090e-03 eta 1:25:58
epoch [12/50] batch [10/25] time 0.051 (2.685) data 0.000 (2.616) loss 0.4355 (0.5613) acc 100.0000 (93.4375) lr 1.8090e-03 eta 0:43:11
epoch [12/50] batch [15/25] time 0.050 (1.807) data 0.000 (1.744) loss 0.5200 (0.5917) acc 87.5000 (91.4583) lr 1.8090e-03 eta 0:28:54
epoch [12/50] batch [20/25] time 0.059 (1.369) data 0.000 (1.308) loss 0.4587 (0.5685) acc 93.7500 (91.8750) lr 1.8090e-03 eta 0:21:47
epoch [12/50] batch [25/25] time 0.052 (1.105) data 0.001 (1.046) loss 0.4199 (0.5647) acc 96.8750 (91.5000) lr 1.7705e-03 eta 0:17:29
epoch [13/50] batch [5/25] time 0.055 (5.290) data 0.000 (5.219) loss 0.6133 (0.5539) acc 93.7500 (92.5000) lr 1.7705e-03 eta 1:23:18
epoch [13/50] batch [10/25] time 0.054 (2.672) data 0.000 (2.610) loss 0.4902 (0.5883) acc 93.7500 (91.2500) lr 1.7705e-03 eta 0:41:52
epoch [13/50] batch [15/25] time 0.059 (1.799) data 0.000 (1.740) loss 0.5996 (0.5619) acc 93.7500 (91.8750) lr 1.7705e-03 eta 0:28:02
epoch [13/50] batch [20/25] time 0.052 (1.362) data 0.001 (1.305) loss 0.4248 (0.5643) acc 96.8750 (92.0312) lr 1.7705e-03 eta 0:21:06
epoch [13/50] batch [25/25] time 0.052 (1.100) data 0.000 (1.044) loss 0.4529 (0.5623) acc 96.8750 (92.0000) lr 1.7290e-03 eta 0:16:57
epoch [14/50] batch [5/25] time 0.052 (5.330) data 0.000 (5.228) loss 0.5176 (0.5155) acc 96.8750 (93.1250) lr 1.7290e-03 eta 1:21:43
epoch [14/50] batch [10/25] time 0.057 (2.693) data 0.001 (2.615) loss 0.5137 (0.5142) acc 90.6250 (92.8125) lr 1.7290e-03 eta 0:41:04
epoch [14/50] batch [15/25] time 0.055 (1.814) data 0.000 (1.743) loss 0.6416 (0.5411) acc 87.5000 (91.4583) lr 1.7290e-03 eta 0:27:30
epoch [14/50] batch [20/25] time 0.049 (1.373) data 0.000 (1.308) loss 0.7090 (0.5595) acc 84.3750 (90.9375) lr 1.7290e-03 eta 0:20:43
epoch [14/50] batch [25/25] time 0.052 (1.109) data 0.000 (1.046) loss 0.7534 (0.5491) acc 84.3750 (91.2500) lr 1.6845e-03 eta 0:16:38
epoch [15/50] batch [5/25] time 0.051 (5.319) data 0.001 (5.226) loss 0.4365 (0.4560) acc 93.7500 (95.0000) lr 1.6845e-03 eta 1:19:20
epoch [15/50] batch [10/25] time 0.051 (2.685) data 0.000 (2.613) loss 0.4783 (0.5055) acc 90.6250 (93.4375) lr 1.6845e-03 eta 0:39:49
epoch [15/50] batch [15/25] time 0.050 (1.807) data 0.000 (1.742) loss 0.6182 (0.5179) acc 90.6250 (93.1250) lr 1.6845e-03 eta 0:26:39
epoch [15/50] batch [20/25] time 0.050 (1.368) data 0.000 (1.307) loss 0.5234 (0.5289) acc 93.7500 (92.3438) lr 1.6845e-03 eta 0:20:04
epoch [15/50] batch [25/25] time 0.051 (1.105) data 0.000 (1.045) loss 0.6538 (0.5202) acc 87.5000 (92.5000) lr 1.6374e-03 eta 0:16:06
epoch [16/50] batch [5/25] time 0.055 (5.379) data 0.000 (5.249) loss 0.4058 (0.4115) acc 96.8750 (96.2500) lr 1.6374e-03 eta 1:18:00
epoch [16/50] batch [10/25] time 0.056 (2.717) data 0.000 (2.625) loss 0.4592 (0.4785) acc 93.7500 (94.0625) lr 1.6374e-03 eta 0:39:10
epoch [16/50] batch [15/25] time 0.060 (1.830) data 0.000 (1.750) loss 0.5454 (0.5183) acc 90.6250 (92.7083) lr 1.6374e-03 eta 0:26:13
epoch [16/50] batch [20/25] time 0.050 (1.385) data 0.001 (1.313) loss 0.5049 (0.5181) acc 90.6250 (92.6562) lr 1.6374e-03 eta 0:19:44
epoch [16/50] batch [25/25] time 0.050 (1.118) data 0.000 (1.050) loss 0.4753 (0.5256) acc 93.7500 (92.3750) lr 1.5878e-03 eta 0:15:50
epoch [17/50] batch [5/25] time 0.051 (5.321) data 0.001 (5.208) loss 0.4724 (0.4859) acc 93.7500 (95.0000) lr 1.5878e-03 eta 1:14:56
epoch [17/50] batch [10/25] time 0.052 (2.687) data 0.000 (2.604) loss 0.5293 (0.4917) acc 90.6250 (94.3750) lr 1.5878e-03 eta 0:37:36
epoch [17/50] batch [15/25] time 0.055 (1.809) data 0.000 (1.736) loss 0.6006 (0.5131) acc 96.8750 (93.5417) lr 1.5878e-03 eta 0:25:10
epoch [17/50] batch [20/25] time 0.054 (1.371) data 0.000 (1.302) loss 0.4507 (0.5377) acc 96.8750 (92.5000) lr 1.5878e-03 eta 0:18:57
epoch [17/50] batch [25/25] time 0.049 (1.107) data 0.000 (1.042) loss 0.3301 (0.5367) acc 96.8750 (92.1250) lr 1.5358e-03 eta 0:15:13
epoch [18/50] batch [5/25] time 0.053 (5.323) data 0.000 (5.232) loss 0.4468 (0.5009) acc 93.7500 (91.2500) lr 1.5358e-03 eta 1:12:44
epoch [18/50] batch [10/25] time 0.052 (2.687) data 0.000 (2.616) loss 0.5586 (0.4837) acc 87.5000 (93.1250) lr 1.5358e-03 eta 0:36:29
epoch [18/50] batch [15/25] time 0.050 (1.809) data 0.000 (1.744) loss 0.6924 (0.5256) acc 84.3750 (92.5000) lr 1.5358e-03 eta 0:24:24
epoch [18/50] batch [20/25] time 0.051 (1.369) data 0.001 (1.308) loss 0.4346 (0.5111) acc 93.7500 (93.1250) lr 1.5358e-03 eta 0:18:22
epoch [18/50] batch [25/25] time 0.050 (1.106) data 0.001 (1.047) loss 0.5083 (0.5089) acc 93.7500 (93.3750) lr 1.4818e-03 eta 0:14:44
epoch [19/50] batch [5/25] time 0.052 (5.328) data 0.000 (5.221) loss 0.5293 (0.4918) acc 87.5000 (92.5000) lr 1.4818e-03 eta 1:10:35
epoch [19/50] batch [10/25] time 0.055 (2.691) data 0.000 (2.611) loss 0.7754 (0.4844) acc 78.1250 (92.5000) lr 1.4818e-03 eta 0:35:25
epoch [19/50] batch [15/25] time 0.054 (1.812) data 0.000 (1.740) loss 0.3857 (0.4702) acc 96.8750 (93.1250) lr 1.4818e-03 eta 0:23:42
epoch [19/50] batch [20/25] time 0.063 (1.373) data 0.000 (1.305) loss 0.6099 (0.4905) acc 87.5000 (92.0312) lr 1.4818e-03 eta 0:17:51
epoch [19/50] batch [25/25] time 0.050 (1.109) data 0.000 (1.044) loss 0.6309 (0.5004) acc 84.3750 (91.6250) lr 1.4258e-03 eta 0:14:19
epoch [20/50] batch [5/25] time 0.054 (5.277) data 0.000 (5.221) loss 0.3501 (0.4205) acc 100.0000 (95.6250) lr 1.4258e-03 eta 1:07:43
epoch [20/50] batch [10/25] time 0.050 (2.664) data 0.000 (2.611) loss 0.3723 (0.4165) acc 100.0000 (95.9375) lr 1.4258e-03 eta 0:33:57
epoch [20/50] batch [15/25] time 0.051 (1.793) data 0.000 (1.740) loss 0.5215 (0.4352) acc 93.7500 (95.6250) lr 1.4258e-03 eta 0:22:42
epoch [20/50] batch [20/25] time 0.051 (1.358) data 0.000 (1.305) loss 0.3806 (0.4520) acc 96.8750 (94.3750) lr 1.4258e-03 eta 0:17:05
epoch [20/50] batch [25/25] time 0.053 (1.096) data 0.000 (1.044) loss 0.6504 (0.4651) acc 84.3750 (93.6250) lr 1.3681e-03 eta 0:13:42
epoch [21/50] batch [5/25] time 0.055 (5.251) data 0.000 (5.194) loss 0.3545 (0.4543) acc 100.0000 (95.0000) lr 1.3681e-03 eta 1:05:11
epoch [21/50] batch [10/25] time 0.052 (2.652) data 0.000 (2.597) loss 0.3362 (0.4224) acc 100.0000 (95.0000) lr 1.3681e-03 eta 0:32:42
epoch [21/50] batch [15/25] time 0.051 (1.785) data 0.000 (1.732) loss 0.4277 (0.4151) acc 96.8750 (95.8333) lr 1.3681e-03 eta 0:21:51
epoch [21/50] batch [20/25] time 0.052 (1.352) data 0.001 (1.299) loss 0.8413 (0.4555) acc 81.2500 (94.0625) lr 1.3681e-03 eta 0:16:26
epoch [21/50] batch [25/25] time 0.051 (1.092) data 0.000 (1.039) loss 0.5449 (0.4641) acc 90.6250 (93.7500) lr 1.3090e-03 eta 0:13:11
epoch [22/50] batch [5/25] time 0.053 (5.342) data 0.000 (5.219) loss 0.3845 (0.4090) acc 96.8750 (95.6250) lr 1.3090e-03 eta 1:04:06
epoch [22/50] batch [10/25] time 0.053 (2.699) data 0.001 (2.610) loss 0.3853 (0.4284) acc 96.8750 (95.0000) lr 1.3090e-03 eta 0:32:09
epoch [22/50] batch [15/25] time 0.052 (1.817) data 0.000 (1.740) loss 0.4670 (0.4517) acc 93.7500 (94.3750) lr 1.3090e-03 eta 0:21:30
epoch [22/50] batch [20/25] time 0.051 (1.376) data 0.000 (1.305) loss 0.3718 (0.4508) acc 100.0000 (94.6875) lr 1.3090e-03 eta 0:16:09
epoch [22/50] batch [25/25] time 0.050 (1.111) data 0.000 (1.044) loss 0.4885 (0.4615) acc 87.5000 (94.0000) lr 1.2487e-03 eta 0:12:57
epoch [23/50] batch [5/25] time 0.050 (5.320) data 0.001 (5.216) loss 0.3950 (0.4022) acc 96.8750 (96.2500) lr 1.2487e-03 eta 1:01:37
epoch [23/50] batch [10/25] time 0.050 (2.686) data 0.001 (2.608) loss 0.3552 (0.4177) acc 96.8750 (94.6875) lr 1.2487e-03 eta 0:30:53
epoch [23/50] batch [15/25] time 0.050 (1.808) data 0.000 (1.739) loss 0.4766 (0.4205) acc 90.6250 (94.5833) lr 1.2487e-03 eta 0:20:38
epoch [23/50] batch [20/25] time 0.050 (1.369) data 0.000 (1.304) loss 0.3369 (0.4305) acc 96.8750 (94.3750) lr 1.2487e-03 eta 0:15:30
epoch [23/50] batch [25/25] time 0.051 (1.105) data 0.000 (1.043) loss 0.4214 (0.4454) acc 96.8750 (94.0000) lr 1.1874e-03 eta 0:12:25
epoch [24/50] batch [5/25] time 0.058 (5.339) data 0.000 (5.224) loss 0.4460 (0.4354) acc 93.7500 (94.3750) lr 1.1874e-03 eta 0:59:36
epoch [24/50] batch [10/25] time 0.054 (2.696) data 0.000 (2.612) loss 0.7520 (0.5275) acc 90.6250 (93.1250) lr 1.1874e-03 eta 0:29:52
epoch [24/50] batch [15/25] time 0.053 (1.815) data 0.001 (1.742) loss 0.5337 (0.5103) acc 96.8750 (93.3333) lr 1.1874e-03 eta 0:19:57
epoch [24/50] batch [20/25] time 0.051 (1.375) data 0.000 (1.306) loss 0.4705 (0.5011) acc 87.5000 (93.1250) lr 1.1874e-03 eta 0:15:00
epoch [24/50] batch [25/25] time 0.053 (1.110) data 0.001 (1.045) loss 0.4248 (0.5239) acc 93.7500 (92.6250) lr 1.1253e-03 eta 0:12:01
epoch [25/50] batch [5/25] time 0.053 (5.324) data 0.001 (5.238) loss 0.6050 (0.4537) acc 93.7500 (94.3750) lr 1.1253e-03 eta 0:57:13
epoch [25/50] batch [10/25] time 0.052 (2.688) data 0.001 (2.619) loss 0.4038 (0.4415) acc 96.8750 (94.3750) lr 1.1253e-03 eta 0:28:40
epoch [25/50] batch [15/25] time 0.057 (1.810) data 0.000 (1.746) loss 0.4272 (0.4772) acc 100.0000 (94.3750) lr 1.1253e-03 eta 0:19:09
epoch [25/50] batch [20/25] time 0.056 (1.372) data 0.000 (1.310) loss 0.5269 (0.4759) acc 93.7500 (93.7500) lr 1.1253e-03 eta 0:14:24
epoch [25/50] batch [25/25] time 0.053 (1.108) data 0.000 (1.048) loss 0.7466 (0.4891) acc 87.5000 (93.3750) lr 1.0628e-03 eta 0:11:32
epoch [26/50] batch [5/25] time 0.051 (5.364) data 0.001 (5.249) loss 0.6113 (0.4605) acc 87.5000 (92.5000) lr 1.0628e-03 eta 0:55:25
epoch [26/50] batch [10/25] time 0.051 (2.708) data 0.000 (2.624) loss 0.3792 (0.4621) acc 96.8750 (92.8125) lr 1.0628e-03 eta 0:27:45
epoch [26/50] batch [15/25] time 0.051 (1.823) data 0.000 (1.750) loss 0.3945 (0.4891) acc 100.0000 (91.6667) lr 1.0628e-03 eta 0:18:31
epoch [26/50] batch [20/25] time 0.051 (1.380) data 0.000 (1.312) loss 0.5420 (0.4999) acc 87.5000 (91.4062) lr 1.0628e-03 eta 0:13:54
epoch [26/50] batch [25/25] time 0.050 (1.114) data 0.000 (1.050) loss 0.4141 (0.4932) acc 90.6250 (91.7500) lr 1.0000e-03 eta 0:11:08
epoch [27/50] batch [5/25] time 0.061 (5.329) data 0.001 (5.224) loss 0.4114 (0.4931) acc 93.7500 (90.6250) lr 1.0000e-03 eta 0:52:51
epoch [27/50] batch [10/25] time 0.056 (2.692) data 0.000 (2.612) loss 0.5557 (0.4777) acc 93.7500 (92.8125) lr 1.0000e-03 eta 0:26:28
epoch [27/50] batch [15/25] time 0.054 (1.813) data 0.000 (1.741) loss 0.5283 (0.4687) acc 96.8750 (93.9583) lr 1.0000e-03 eta 0:17:40
epoch [27/50] batch [20/25] time 0.055 (1.374) data 0.000 (1.306) loss 0.7271 (0.4955) acc 84.3750 (92.9688) lr 1.0000e-03 eta 0:13:16
epoch [27/50] batch [25/25] time 0.051 (1.109) data 0.000 (1.045) loss 0.6729 (0.4997) acc 87.5000 (92.7500) lr 9.3721e-04 eta 0:10:37
epoch [28/50] batch [5/25] time 0.051 (5.324) data 0.000 (5.219) loss 0.4131 (0.4573) acc 90.6250 (93.1250) lr 9.3721e-04 eta 0:50:34
epoch [28/50] batch [10/25] time 0.052 (2.687) data 0.000 (2.610) loss 0.3848 (0.4227) acc 100.0000 (96.2500) lr 9.3721e-04 eta 0:25:18
epoch [28/50] batch [15/25] time 0.052 (1.809) data 0.000 (1.740) loss 0.4248 (0.4445) acc 96.8750 (94.7917) lr 9.3721e-04 eta 0:16:53
epoch [28/50] batch [20/25] time 0.056 (1.371) data 0.000 (1.305) loss 0.4731 (0.4738) acc 96.8750 (94.0625) lr 9.3721e-04 eta 0:12:40
epoch [28/50] batch [25/25] time 0.068 (1.108) data 0.000 (1.044) loss 0.6182 (0.4712) acc 93.7500 (94.0000) lr 8.7467e-04 eta 0:10:09
epoch [29/50] batch [5/25] time 0.049 (5.321) data 0.000 (5.217) loss 0.4302 (0.4026) acc 90.6250 (95.6250) lr 8.7467e-04 eta 0:48:19
epoch [29/50] batch [10/25] time 0.050 (2.686) data 0.001 (2.609) loss 0.3796 (0.4359) acc 93.7500 (94.0625) lr 8.7467e-04 eta 0:24:10
epoch [29/50] batch [15/25] time 0.051 (1.808) data 0.000 (1.739) loss 0.5708 (0.4427) acc 90.6250 (94.3750) lr 8.7467e-04 eta 0:16:07
epoch [29/50] batch [20/25] time 0.050 (1.369) data 0.000 (1.304) loss 0.2686 (0.4529) acc 100.0000 (93.9062) lr 8.7467e-04 eta 0:12:05
epoch [29/50] batch [25/25] time 0.051 (1.105) data 0.000 (1.044) loss 0.5596 (0.4559) acc 90.6250 (93.8750) lr 8.1262e-04 eta 0:09:40
epoch [30/50] batch [5/25] time 0.053 (5.290) data 0.000 (5.217) loss 0.3445 (0.3872) acc 96.8750 (96.8750) lr 8.1262e-04 eta 0:45:50
epoch [30/50] batch [10/25] time 0.051 (2.670) data 0.000 (2.609) loss 0.4260 (0.4321) acc 96.8750 (94.6875) lr 8.1262e-04 eta 0:22:55
epoch [30/50] batch [15/25] time 0.049 (1.798) data 0.000 (1.739) loss 0.4788 (0.4371) acc 93.7500 (94.3750) lr 8.1262e-04 eta 0:15:16
epoch [30/50] batch [20/25] time 0.052 (1.361) data 0.000 (1.304) loss 0.4561 (0.4359) acc 96.8750 (94.8438) lr 8.1262e-04 eta 0:11:27
epoch [30/50] batch [25/25] time 0.052 (1.099) data 0.000 (1.044) loss 0.4524 (0.4268) acc 87.5000 (94.6250) lr 7.5131e-04 eta 0:09:09
epoch [31/50] batch [5/25] time 0.053 (5.240) data 0.000 (5.187) loss 0.3484 (0.3618) acc 93.7500 (96.8750) lr 7.5131e-04 eta 0:43:13
epoch [31/50] batch [10/25] time 0.051 (2.646) data 0.000 (2.593) loss 0.4380 (0.4199) acc 93.7500 (95.3125) lr 7.5131e-04 eta 0:21:36
epoch [31/50] batch [15/25] time 0.052 (1.781) data 0.000 (1.729) loss 0.3521 (0.4236) acc 96.8750 (95.0000) lr 7.5131e-04 eta 0:14:23
epoch [31/50] batch [20/25] time 0.050 (1.349) data 0.000 (1.297) loss 0.4141 (0.4247) acc 96.8750 (95.1562) lr 7.5131e-04 eta 0:10:47
epoch [31/50] batch [25/25] time 0.051 (1.089) data 0.001 (1.038) loss 0.4077 (0.4249) acc 96.8750 (95.0000) lr 6.9098e-04 eta 0:08:37
epoch [32/50] batch [5/25] time 0.053 (5.352) data 0.001 (5.244) loss 0.5137 (0.4443) acc 90.6250 (93.7500) lr 6.9098e-04 eta 0:41:55
epoch [32/50] batch [10/25] time 0.054 (2.703) data 0.000 (2.622) loss 0.4729 (0.4716) acc 93.7500 (92.5000) lr 6.9098e-04 eta 0:20:56
epoch [32/50] batch [15/25] time 0.052 (1.819) data 0.001 (1.748) loss 0.3354 (0.4423) acc 96.8750 (94.1667) lr 6.9098e-04 eta 0:13:56
epoch [32/50] batch [20/25] time 0.050 (1.378) data 0.000 (1.311) loss 0.4341 (0.4404) acc 93.7500 (94.0625) lr 6.9098e-04 eta 0:10:26
epoch [32/50] batch [25/25] time 0.051 (1.113) data 0.000 (1.049) loss 0.3647 (0.4544) acc 93.7500 (93.3750) lr 6.3188e-04 eta 0:08:20
epoch [33/50] batch [5/25] time 0.050 (5.324) data 0.000 (5.204) loss 0.3044 (0.4572) acc 96.8750 (93.1250) lr 6.3188e-04 eta 0:39:29
epoch [33/50] batch [10/25] time 0.052 (2.688) data 0.000 (2.602) loss 0.2834 (0.4519) acc 100.0000 (94.0625) lr 6.3188e-04 eta 0:19:42
epoch [33/50] batch [15/25] time 0.054 (1.810) data 0.000 (1.735) loss 0.3643 (0.4636) acc 100.0000 (94.1667) lr 6.3188e-04 eta 0:13:07
epoch [33/50] batch [20/25] time 0.049 (1.371) data 0.000 (1.301) loss 0.5088 (0.4532) acc 93.7500 (94.6875) lr 6.3188e-04 eta 0:09:49
epoch [33/50] batch [25/25] time 0.052 (1.107) data 0.000 (1.041) loss 0.2822 (0.4422) acc 100.0000 (94.8750) lr 5.7422e-04 eta 0:07:50
epoch [34/50] batch [5/25] time 0.051 (5.325) data 0.000 (5.217) loss 0.6885 (0.4596) acc 87.5000 (93.1250) lr 5.7422e-04 eta 0:37:16
epoch [34/50] batch [10/25] time 0.053 (2.689) data 0.000 (2.609) loss 0.4263 (0.4291) acc 90.6250 (94.0625) lr 5.7422e-04 eta 0:18:35
epoch [34/50] batch [15/25] time 0.057 (1.810) data 0.000 (1.739) loss 0.6455 (0.4614) acc 84.3750 (93.3333) lr 5.7422e-04 eta 0:12:21
epoch [34/50] batch [20/25] time 0.051 (1.370) data 0.000 (1.304) loss 0.5356 (0.4837) acc 87.5000 (92.3438) lr 5.7422e-04 eta 0:09:14
epoch [34/50] batch [25/25] time 0.052 (1.106) data 0.000 (1.044) loss 0.4670 (0.4695) acc 90.6250 (92.7500) lr 5.1825e-04 eta 0:07:22
epoch [35/50] batch [5/25] time 0.052 (5.329) data 0.000 (5.241) loss 0.4182 (0.3977) acc 96.8750 (96.2500) lr 5.1825e-04 eta 0:35:05
epoch [35/50] batch [10/25] time 0.056 (2.693) data 0.000 (2.621) loss 0.3179 (0.4020) acc 96.8750 (96.5625) lr 5.1825e-04 eta 0:17:30
epoch [35/50] batch [15/25] time 0.055 (1.814) data 0.000 (1.747) loss 0.3955 (0.4187) acc 90.6250 (95.4167) lr 5.1825e-04 eta 0:11:38
epoch [35/50] batch [20/25] time 0.055 (1.374) data 0.000 (1.310) loss 0.3342 (0.4076) acc 93.7500 (95.3125) lr 5.1825e-04 eta 0:08:42
epoch [35/50] batch [25/25] time 0.050 (1.110) data 0.000 (1.048) loss 0.3225 (0.4063) acc 96.8750 (95.3750) lr 4.6417e-04 eta 0:06:56
epoch [36/50] batch [5/25] time 0.051 (5.320) data 0.000 (5.222) loss 0.3428 (0.3822) acc 93.7500 (95.0000) lr 4.6417e-04 eta 0:32:48
epoch [36/50] batch [10/25] time 0.049 (2.685) data 0.000 (2.611) loss 0.4016 (0.3692) acc 96.8750 (96.2500) lr 4.6417e-04 eta 0:16:20
epoch [36/50] batch [15/25] time 0.049 (1.808) data 0.000 (1.741) loss 0.4243 (0.3834) acc 93.7500 (95.4167) lr 4.6417e-04 eta 0:10:50
epoch [36/50] batch [20/25] time 0.056 (1.369) data 0.000 (1.306) loss 0.3801 (0.3767) acc 96.8750 (95.9375) lr 4.6417e-04 eta 0:08:06
epoch [36/50] batch [25/25] time 0.057 (1.106) data 0.000 (1.044) loss 0.3948 (0.3805) acc 96.8750 (95.8750) lr 4.1221e-04 eta 0:06:27
epoch [37/50] batch [5/25] time 0.051 (5.268) data 0.000 (5.213) loss 0.3811 (0.3774) acc 96.8750 (95.0000) lr 4.1221e-04 eta 0:30:17
epoch [37/50] batch [10/25] time 0.059 (2.660) data 0.000 (2.607) loss 0.2812 (0.3653) acc 100.0000 (95.9375) lr 4.1221e-04 eta 0:15:04
epoch [37/50] batch [15/25] time 0.049 (1.790) data 0.000 (1.738) loss 0.6030 (0.3939) acc 84.3750 (94.5833) lr 4.1221e-04 eta 0:09:59
epoch [37/50] batch [20/25] time 0.050 (1.355) data 0.000 (1.303) loss 0.3530 (0.3974) acc 96.8750 (94.3750) lr 4.1221e-04 eta 0:07:27
epoch [37/50] batch [25/25] time 0.063 (1.095) data 0.000 (1.043) loss 0.2368 (0.4204) acc 100.0000 (93.6250) lr 3.6258e-04 eta 0:05:55
epoch [38/50] batch [5/25] time 0.057 (5.317) data 0.001 (5.218) loss 0.3350 (0.4334) acc 100.0000 (95.0000) lr 3.6258e-04 eta 0:28:21
epoch [38/50] batch [10/25] time 0.053 (2.686) data 0.001 (2.609) loss 0.4434 (0.4219) acc 93.7500 (95.0000) lr 3.6258e-04 eta 0:14:06
epoch [38/50] batch [15/25] time 0.050 (1.808) data 0.000 (1.740) loss 0.4822 (0.4308) acc 93.7500 (94.5833) lr 3.6258e-04 eta 0:09:20
epoch [38/50] batch [20/25] time 0.052 (1.369) data 0.000 (1.305) loss 0.4849 (0.4273) acc 93.7500 (94.6875) lr 3.6258e-04 eta 0:06:57
epoch [38/50] batch [25/25] time 0.057 (1.106) data 0.000 (1.044) loss 0.4175 (0.4123) acc 93.7500 (95.2500) lr 3.1545e-04 eta 0:05:31
epoch [39/50] batch [5/25] time 0.051 (5.351) data 0.001 (5.232) loss 0.5459 (0.3815) acc 90.6250 (96.2500) lr 3.1545e-04 eta 0:26:18
epoch [39/50] batch [10/25] time 0.052 (2.702) data 0.001 (2.616) loss 0.3589 (0.4548) acc 93.7500 (93.1250) lr 3.1545e-04 eta 0:13:03
epoch [39/50] batch [15/25] time 0.051 (1.818) data 0.000 (1.744) loss 0.4739 (0.4443) acc 93.7500 (93.7500) lr 3.1545e-04 eta 0:08:38
epoch [39/50] batch [20/25] time 0.050 (1.376) data 0.000 (1.308) loss 0.4009 (0.4216) acc 93.7500 (94.2188) lr 3.1545e-04 eta 0:06:25
epoch [39/50] batch [25/25] time 0.050 (1.111) data 0.000 (1.046) loss 0.6704 (0.4253) acc 87.5000 (94.1250) lr 2.7103e-04 eta 0:05:05
epoch [40/50] batch [5/25] time 0.057 (5.310) data 0.001 (5.239) loss 0.5723 (0.4865) acc 93.7500 (93.7500) lr 2.7103e-04 eta 0:23:53
epoch [40/50] batch [10/25] time 0.053 (2.683) data 0.000 (2.620) loss 0.3252 (0.4556) acc 96.8750 (93.7500) lr 2.7103e-04 eta 0:11:50
epoch [40/50] batch [15/25] time 0.051 (1.806) data 0.000 (1.747) loss 0.6016 (0.4599) acc 87.5000 (93.3333) lr 2.7103e-04 eta 0:07:49
epoch [40/50] batch [20/25] time 0.051 (1.367) data 0.001 (1.310) loss 0.3994 (0.4340) acc 96.8750 (94.0625) lr 2.7103e-04 eta 0:05:48
epoch [40/50] batch [25/25] time 0.051 (1.104) data 0.000 (1.048) loss 0.5200 (0.4299) acc 90.6250 (94.0000) lr 2.2949e-04 eta 0:04:36
epoch [41/50] batch [5/25] time 0.051 (5.307) data 0.000 (5.218) loss 0.3608 (0.4703) acc 93.7500 (92.5000) lr 2.2949e-04 eta 0:21:40
epoch [41/50] batch [10/25] time 0.053 (2.681) data 0.000 (2.609) loss 0.4204 (0.4728) acc 90.6250 (92.1875) lr 2.2949e-04 eta 0:10:43
epoch [41/50] batch [15/25] time 0.053 (1.805) data 0.000 (1.740) loss 0.2351 (0.4232) acc 100.0000 (94.5833) lr 2.2949e-04 eta 0:07:04
epoch [41/50] batch [20/25] time 0.052 (1.367) data 0.000 (1.305) loss 0.5244 (0.4274) acc 87.5000 (94.0625) lr 2.2949e-04 eta 0:05:14
epoch [41/50] batch [25/25] time 0.050 (1.104) data 0.000 (1.044) loss 0.4558 (0.4314) acc 93.7500 (94.3750) lr 1.9098e-04 eta 0:04:08
epoch [42/50] batch [5/25] time 0.051 (5.287) data 0.000 (5.198) loss 0.4272 (0.4542) acc 93.7500 (93.7500) lr 1.9098e-04 eta 0:19:23
epoch [42/50] batch [10/25] time 0.051 (2.669) data 0.000 (2.599) loss 0.5229 (0.4605) acc 93.7500 (93.7500) lr 1.9098e-04 eta 0:09:33
epoch [42/50] batch [15/25] time 0.051 (1.796) data 0.000 (1.733) loss 0.4001 (0.4556) acc 93.7500 (93.1250) lr 1.9098e-04 eta 0:06:17
epoch [42/50] batch [20/25] time 0.051 (1.361) data 0.001 (1.299) loss 0.4749 (0.4431) acc 93.7500 (93.5938) lr 1.9098e-04 eta 0:04:38
epoch [42/50] batch [25/25] time 0.051 (1.099) data 0.000 (1.040) loss 0.5073 (0.4344) acc 93.7500 (93.8750) lr 1.5567e-04 eta 0:03:39
epoch [43/50] batch [5/25] time 0.050 (5.291) data 0.000 (5.206) loss 0.2944 (0.4341) acc 96.8750 (95.0000) lr 1.5567e-04 eta 0:17:11
epoch [43/50] batch [10/25] time 0.051 (2.672) data 0.001 (2.603) loss 0.4158 (0.4353) acc 90.6250 (94.6875) lr 1.5567e-04 eta 0:08:27
epoch [43/50] batch [15/25] time 0.053 (1.799) data 0.000 (1.735) loss 0.3994 (0.4062) acc 93.7500 (95.2083) lr 1.5567e-04 eta 0:05:32
epoch [43/50] batch [20/25] time 0.053 (1.363) data 0.000 (1.302) loss 0.3560 (0.4014) acc 100.0000 (95.4688) lr 1.5567e-04 eta 0:04:05
epoch [43/50] batch [25/25] time 0.053 (1.101) data 0.000 (1.041) loss 0.4880 (0.4136) acc 93.7500 (95.2500) lr 1.2369e-04 eta 0:03:12
epoch [44/50] batch [5/25] time 0.065 (5.325) data 0.001 (5.205) loss 0.4729 (0.4543) acc 93.7500 (94.3750) lr 1.2369e-04 eta 0:15:05
epoch [44/50] batch [10/25] time 0.051 (2.688) data 0.000 (2.603) loss 0.2808 (0.4026) acc 100.0000 (96.2500) lr 1.2369e-04 eta 0:07:23
epoch [44/50] batch [15/25] time 0.050 (1.808) data 0.000 (1.735) loss 0.3101 (0.3900) acc 96.8750 (96.4583) lr 1.2369e-04 eta 0:04:49
epoch [44/50] batch [20/25] time 0.052 (1.369) data 0.000 (1.301) loss 0.4778 (0.4102) acc 93.7500 (95.9375) lr 1.2369e-04 eta 0:03:32
epoch [44/50] batch [25/25] time 0.053 (1.106) data 0.000 (1.041) loss 0.3401 (0.4034) acc 100.0000 (96.3750) lr 9.5173e-05 eta 0:02:45
epoch [45/50] batch [5/25] time 0.053 (5.295) data 0.001 (5.205) loss 0.2681 (0.3896) acc 96.8750 (95.0000) lr 9.5173e-05 eta 0:12:47
epoch [45/50] batch [10/25] time 0.051 (2.673) data 0.000 (2.603) loss 0.3394 (0.3750) acc 96.8750 (95.6250) lr 9.5173e-05 eta 0:06:14
epoch [45/50] batch [15/25] time 0.050 (1.800) data 0.000 (1.735) loss 0.3665 (0.3835) acc 96.8750 (95.4167) lr 9.5173e-05 eta 0:04:02
epoch [45/50] batch [20/25] time 0.049 (1.362) data 0.000 (1.301) loss 0.4766 (0.3865) acc 96.8750 (95.4688) lr 9.5173e-05 eta 0:02:57
epoch [45/50] batch [25/25] time 0.051 (1.100) data 0.000 (1.041) loss 0.3162 (0.3869) acc 96.8750 (94.8750) lr 7.0224e-05 eta 0:02:17
epoch [46/50] batch [5/25] time 0.054 (5.333) data 0.000 (5.227) loss 0.3267 (0.3452) acc 96.8750 (96.2500) lr 7.0224e-05 eta 0:10:39
epoch [46/50] batch [10/25] time 0.054 (2.695) data 0.001 (2.613) loss 0.4429 (0.3615) acc 93.7500 (96.2500) lr 7.0224e-05 eta 0:05:09
epoch [46/50] batch [15/25] time 0.052 (1.814) data 0.000 (1.742) loss 0.3525 (0.3705) acc 93.7500 (95.8333) lr 7.0224e-05 eta 0:03:19
epoch [46/50] batch [20/25] time 0.062 (1.374) data 0.000 (1.307) loss 0.4856 (0.3800) acc 93.7500 (95.4688) lr 7.0224e-05 eta 0:02:24
epoch [46/50] batch [25/25] time 0.052 (1.109) data 0.000 (1.045) loss 0.3411 (0.3778) acc 100.0000 (95.6250) lr 4.8943e-05 eta 0:01:50
epoch [47/50] batch [5/25] time 0.051 (5.322) data 0.001 (5.232) loss 0.3875 (0.3379) acc 93.7500 (96.8750) lr 4.8943e-05 eta 0:08:25
epoch [47/50] batch [10/25] time 0.052 (2.687) data 0.000 (2.616) loss 0.3169 (0.4020) acc 96.8750 (95.0000) lr 4.8943e-05 eta 0:04:01
epoch [47/50] batch [15/25] time 0.051 (1.809) data 0.000 (1.744) loss 0.3745 (0.4089) acc 100.0000 (95.6250) lr 4.8943e-05 eta 0:02:33
epoch [47/50] batch [20/25] time 0.051 (1.369) data 0.000 (1.308) loss 0.3235 (0.4073) acc 100.0000 (95.9375) lr 4.8943e-05 eta 0:01:49
epoch [47/50] batch [25/25] time 0.049 (1.106) data 0.000 (1.046) loss 0.4995 (0.4202) acc 93.7500 (95.5000) lr 3.1417e-05 eta 0:01:22
epoch [48/50] batch [5/25] time 0.053 (5.336) data 0.001 (5.223) loss 0.3967 (0.4132) acc 93.7500 (95.0000) lr 3.1417e-05 eta 0:06:13
epoch [48/50] batch [10/25] time 0.054 (2.695) data 0.000 (2.611) loss 0.4954 (0.4320) acc 90.6250 (94.6875) lr 3.1417e-05 eta 0:02:55
epoch [48/50] batch [15/25] time 0.051 (1.814) data 0.000 (1.741) loss 0.4314 (0.4130) acc 96.8750 (95.2083) lr 3.1417e-05 eta 0:01:48
epoch [48/50] batch [20/25] time 0.052 (1.374) data 0.000 (1.306) loss 0.4807 (0.4213) acc 93.7500 (95.1562) lr 3.1417e-05 eta 0:01:15
epoch [48/50] batch [25/25] time 0.050 (1.109) data 0.000 (1.045) loss 0.5439 (0.4163) acc 90.6250 (95.2500) lr 1.7713e-05 eta 0:00:55
epoch [49/50] batch [5/25] time 0.052 (5.330) data 0.000 (5.212) loss 0.4460 (0.3889) acc 93.7500 (94.3750) lr 1.7713e-05 eta 0:03:59
epoch [49/50] batch [10/25] time 0.065 (2.692) data 0.001 (2.606) loss 0.4419 (0.4071) acc 96.8750 (95.0000) lr 1.7713e-05 eta 0:01:47
epoch [49/50] batch [15/25] time 0.054 (1.813) data 0.000 (1.737) loss 0.4524 (0.4290) acc 90.6250 (93.7500) lr 1.7713e-05 eta 0:01:03
epoch [49/50] batch [20/25] time 0.051 (1.373) data 0.000 (1.303) loss 0.3757 (0.4381) acc 96.8750 (93.5938) lr 1.7713e-05 eta 0:00:41
epoch [49/50] batch [25/25] time 0.050 (1.109) data 0.000 (1.042) loss 0.3574 (0.4489) acc 96.8750 (93.3750) lr 7.8853e-06 eta 0:00:27
epoch [50/50] batch [5/25] time 0.054 (5.274) data 0.000 (5.221) loss 0.3589 (0.3849) acc 96.8750 (96.2500) lr 7.8853e-06 eta 0:01:45
epoch [50/50] batch [10/25] time 0.052 (2.663) data 0.000 (2.610) loss 0.3267 (0.3788) acc 96.8750 (96.2500) lr 7.8853e-06 eta 0:00:39
epoch [50/50] batch [15/25] time 0.051 (1.793) data 0.000 (1.740) loss 0.3691 (0.3908) acc 96.8750 (95.8333) lr 7.8853e-06 eta 0:00:17
epoch [50/50] batch [20/25] time 0.050 (1.357) data 0.000 (1.305) loss 0.4172 (0.3933) acc 96.8750 (95.7812) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [25/25] time 0.052 (1.096) data 0.000 (1.044) loss 0.3887 (0.4138) acc 93.7500 (95.1250) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output_flowers_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3\prompt_learner\model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 1,053
* correct: 1,030
* accuracy: 97.8%
* error: 2.2%
* macro_f1: 97.8%
Elapsed: 0:24:00
